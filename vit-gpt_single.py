!pip install
!pip install evaluate
!pip install datasets
!pip install nltk
!pip install transformers
!pip install rouge_score


# -*- coding: utf-8 -*-
"""transformer5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j_Pmg78NZpVjzLcWjPbfYMJIzz7_zl_b
"""



# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'flickr8k:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F623289%2F1111676%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240830%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240830T061807Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da3fe13ab9d08b158acb872855d81962d7b33991477a618ed5144d89689e36b48767a4512b097521b69912ede2b8937b23c39901296d699bea7b5582e863c42fd85e3e6b1b26044a9d6b3298554e491cbd09eec36c04eccc50a050d451246eea52b12934d3e39b4789beee8fe05cedec03033c7121ae8ae7f633f1c1506096313a993ca9009936babadf04f79bd643e821dd69a31a86f92d76d5ddc958c31abe7501e7f3f0e436fefdc3aebb8c40e30a87c3a8836c47c6eb5d952554c71e620797b29ea771f5bf2f261c710d8d7de4a309bfa23740ac4c41cd4969cba7c0a446ce0b3ce002680287be58138dea6696de4b0462f1270400930dc96b08d92b3d3f2'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

!pip install
!pip install evaluate
!pip install datasets
!pip install nltk
!pip install transformers
!pip install rouge_score

pip install
pip install evaluate
pip install datasets
pip install nltk
pip install transformers
pip install rouge_score

!pip install transformers

import torch
import numpy as np
from torch.utils.data import DataLoader
from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor
from PIL import Image
import os
import cv2
import matplotlib.pyplot as plt
from transformers import pipeline
import evaluate
from datasets import Dataset
from nltk.translate.bleu_score import corpus_bleu
import pandas as pd

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pre-trained Vision Transformer (ViT) model and GPT-2 model
image_encoder_model = "google/vit-base-patch16-224-in21k"
text_decode_model = "gpt2"
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decode_model)
model.to(device)

# Load feature extractor and tokenizer for the respective models
feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)
tokenizer = AutoTokenizer.from_pretrained(text_decode_model)
tokenizer.pad_token = tokenizer.eos_token

# Set model configurations
model.config.eos_token_id = tokenizer.eos_token_id
model.config.decoder_start_token_id = tokenizer.bos_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.max_length = 20
model.config.min_length = 6
model.config.length_penalty = 1.3
model.config.num_beams = 3

# Load the captions file into a pandas DataFrame
CAPTIONS_FILE = "../input/flickr8k/captions.txt"
IMAGES_DIR = '../input/flickr8k/Images'

df = pd.read_csv(CAPTIONS_FILE)
df['image'] = df['image'].apply(lambda x: f'{IMAGES_DIR}/{x}')

# Split the dataset
total_length = len(df)
train_size = int(total_length * 0.7)
val_size = int(total_length * 0.2)
test_size = total_length - train_size - val_size

train_df = df.iloc[:train_size, :]
val_df = df.iloc[train_size:train_size + val_size, :]
test_df = df.iloc[train_size + val_size:, :]

# Convert to HuggingFace Dataset
processed_train = Dataset.from_pandas(train_df)
processed_val = Dataset.from_pandas(val_df)
processed_test = Dataset.from_pandas(test_df)

# Dataset class definition
class Flickr8kDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, feature_extractor, tokenizer, max_target_length=128):
        self.dataset = dataset
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image_path = self.dataset[idx]['image']
        caption = self.dataset[idx]['caption']

        # Load the image and preprocess it using the feature extractor
        image = Image.open(image_path).convert("RGB")
        encoder_input = self.feature_extractor(images=image, return_tensors="pt")

        # Tokenize the caption
        labels = self.tokenizer(caption, max_length=self.max_target_length, padding="max_length", truncation=True, return_tensors="pt")

        # Move tensors to GPU
        pixel_values = encoder_input.pixel_values.squeeze(0)
        labels = labels.input_ids.flatten()

        return {"labels": labels, "pixel_values": pixel_values}

print(df)

# Early stopping class
class EarlyStopping:
    def __init__(self, patience=3, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
import time

# Function to train the model with timing per epoch
def train_model(model, train_loader, val_loader, optimizer, num_epochs=10, patience=3):
    early_stopping = EarlyStopping(patience=patience)
    model.train()

    for epoch in range(num_epochs):
        start_time = time.time()  # Start the timer at the beginning of the epoch

        total_loss = 0
        for batch in train_loader:
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(pixel_values=pixel_values, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)

        # Calculate the time taken for this epoch
        epoch_time = time.time() - start_time

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {epoch_time:.2f} seconds")

        # Validation after each epoch
        val_loss = evaluate_model(model, val_loader)

        # Early stopping check
        early_stopping(val_loss)
        if early_stopping.early_stop:
            print("Early stopping triggered")
            break

    # Save the trained model
    model.save_pretrained("/content/vitgpt-2")

# Function to evaluate the model
def evaluate_model(model, val_loader):
    model.eval()
    total_loss = 0
    for batch in val_loader:
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        with torch.no_grad():
            outputs = model(pixel_values=pixel_values, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()

    avg_loss = total_loss / len(val_loader)
    print(f"Validation Loss: {avg_loss:.4f}")
    model.train()
    return avg_loss

# Function to test the model and generate captions
def test_model(model, test_loader):
    model.eval()
    predictions = []
    references = []

    for batch in test_loader:
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        with torch.no_grad():
            output_ids = model.generate(pixel_values, max_length=20, num_beams=3, early_stopping=True)
            preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
            refs = tokenizer.batch_decode(labels, skip_special_tokens=True)
            predictions.extend(preds)
            references.extend(refs)

    return predictions, references

# Assuming train_data, val_data, and test_data are instances of Flickr8kDataset
train_loader = DataLoader(Flickr8kDataset(processed_train, feature_extractor, tokenizer), batch_size=16, shuffle=True, num_workers=0)
val_loader = DataLoader(Flickr8kDataset(processed_val, feature_extractor, tokenizer), batch_size=16, shuffle=False, num_workers=0)
test_loader = DataLoader(Flickr8kDataset(processed_test, feature_extractor, tokenizer), batch_size=16, shuffle=False, num_workers=0)

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Train the model with early stopping
train_model(model, train_loader, val_loader, optimizer, num_epochs=10, patience=3)



# Test the model
predictions, references = test_model(model, test_loader)

# BLEU score calculation function
def compute_bleu(predictions, references):
    references = [[ref] for ref in references]
    bleu_score_1 = corpus_bleu(references, predictions, weights=(0.3, 0.3, 0.3, 0))
    bleu_score_2 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))

    return {'bleu_1': bleu_score_1, 'bleu_2': bleu_score_2}

# ROUGE score calculation function
def compute_rouge(predictions, references):
    rouge_metric = evaluate.load("rouge")
    results = rouge_metric.compute(predictions=predictions, references=references)
    rouge_1 = results['rouge1']
    rouge_2 = results['rouge2']
    rouge_l = results['rougeL']
    return rouge_1, rouge_2, rouge_l

# Get BLEU and ROUGE scores
def get_scores(correct_caps, gen_caption):
    # Debugging: Print the correct and generated captions
    print(f"Correct captions: {correct_caps}")
    print(f"Generated caption: {gen_caption}")

    # Split the reference caption into individual sentences for BLEU calculation
    correct_caps_list = [correct_caps.split()]  # Tokenize the reference captions
    predictions = [gen_caption.split()]  # Tokenize the generated captions

    # Debugging: Print tokenized reference and generated captions
    print(f"Tokenized reference captions: {correct_caps_list}")
    print(f"Tokenized generated caption: {predictions}")

    # BLEU 점수 계산
    bleu_scores = compute_bleu(predictions, correct_caps_list)

    # ROUGE 점수 계산
    rouge_1, rouge_2, rouge_l = compute_rouge([gen_caption], correct_caps_list)

    return bleu_scores, rouge_1, rouge_2, rouge_l

# Display image with generated captions and scores
def image_display(filepath, correct_caps, gen_caption, bleu_scores=None, rouge_1=None, rouge_2=None, rouge_l=None, font_size=7, dpi=300, save_img=False):
    img_color = cv2.imread(filepath, 1)
    plt.axis("off")
    plt.imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))

    correct_caps_string = f"> Correct Captions:\n{correct_caps}"
    generated_caption_string = f"> Generated Caption:\n{gen_caption}"

    scores_string = "> Scores:\n"
    if bleu_scores is not None:
        scores_string += f"BLEU-1: {bleu_scores['bleu_1']:.4f} | "
        scores_string += f"BLEU-2: {bleu_scores['bleu_2']:.4f} | "
    if rouge_1 is not None and rouge_2 is not None and rouge_l is not None:
        scores_string += f"ROUGE-1: {rouge_1:.4f} | ROUGE-2: {rouge_2:.4f} | ROUGE-L: {rouge_l:.4f}"

    title_string = f"{correct_caps_string}\n\n{generated_caption_string}\n\n{scores_string.strip('| ')}"
    plt.title(title_string, fontsize=font_size, wrap=True, loc='left')

    if save_img:
        plt.savefig(f"{os.path.splitext(os.path.basename(filepath))[0]}_generation", bbox_inches='tight', dpi=dpi)
    plt.show()

# Test and display captions for specific images
def get_caption_for_image(filepath, correct_caps, font_size=7, dpi=300, save_img=False):
    model.eval()
    image = Image.open(filepath).convert("RGB")
    encoder_input = feature_extractor(images=image, return_tensors="pt").pixel_values.to(device)

    with torch.no_grad():
        output_ids = model.generate(encoder_input, max_length=20, num_beams=3, early_stopping=True)
        gen_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    bleu_scores, rouge_1, rouge_2, rouge_l = get_scores(correct_caps, gen_caption)
    image_display(filepath, correct_caps, gen_caption, bleu_scores, rouge_1, rouge_2, rouge_l, font_size, dpi, save_img)


import numpy as np

# Lists to store the BLEU and ROUGE scores
bleu_scores_list = []
rouge_1_list = []
rouge_2_list = []
rouge_l_list = []

# Loop through the test dataset
for i in range(len(grouped_test_ds)):
    instance = grouped_test_ds.__getitem__(i)
    image_path = instance['image']
    caption = instance['caption']

    # Generate caption for the image
    gen_caption = generate_caption(image_path)

    # Get BLEU and ROUGE scores
    bleu_scores, rouge_1, rouge_2, rouge_l = get_scores(caption, gen_caption)

    # Save the scores to the lists
    bleu_scores_list.append(bleu_scores)
    rouge_1_list.append(rouge_1)
    rouge_2_list.append(rouge_2)
    rouge_l_list.append(rouge_l)

# Calculate the average of each score
avg_bleu_1 = np.mean([score['bleu_1'] for score in bleu_scores_list])
avg_bleu_2 = np.mean([score['bleu_2'] for score in bleu_scores_list])
avg_rouge_1 = np.mean(rouge_1_list)
avg_rouge_2 = np.mean(rouge_2_list)
avg_rouge_l = np.mean(rouge_l_list)

# Print the average scores
print(f"\nAverage BLEU-1: {avg_bleu_1:.4f}")
print(f"Average BLEU-2: {avg_bleu_2:.4f}")
print(f"Average ROUGE-1: {avg_rouge_1:.4f}")
print(f"Average ROUGE-2: {avg_rouge_2:.4f}")
print(f"Average ROUGE-L: {avg_rouge_l:.4f}")