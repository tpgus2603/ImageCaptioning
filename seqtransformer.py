# -*- coding: utf-8 -*-
"""transformer3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11-gIH2zfmnfNUv4A2_bkl6rG4WFWjsUf
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'flickr8k:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F623289%2F1111676%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240815%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240815T035313Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9c9827788ac5da87c320cec406cb22c1150b0f67c1d74a5b2b18bad1e51075e67ff14290e4193c8198b5dd12f63617e952c9c9db4efb3b7a8d12b90123d4c32cfe72d4466ee99d1b59a5cc3031c139fd08f95b2593c7895131b154849a078287da2a521c4c47eac911aa2239a6cdd7a6bf696329009b9c9bcc6a5f78986ff7e4e3b2322adbc2a5fa8561ded10024d298d04eff99169e246f28714df8495f9086c69c40399c881d2760d299c6f0d56ccc73684d5768f808b1a8feca66f0b72f68dd7aab49407b68f508f50c9421932dec8f25b71099edf41115f710e0cd315167d9f4b299d9cdbbb86602bdef3f7764a1754c8940ec757a945b85a8ef336ca092'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets
!pip install accelerate -U
!pip install transformers[torch] -U
!pip install evaluate
import transformers
from PIL import Image
import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from torch.utils.data import DataLoader
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil
import pandas as pd
import re
import torch
from datasets import Dataset
from transformers import VisionEncoderDecoderModel, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback, default_data_collator
from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, default_data_collator
import evaluate
import numpy as np
import matplotlib.pyplot as plt
import cv2
from transformers import pipeline
import nltk

nltk.download('punkt')
# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pre-trained Vision Transformer (ViT) model and GPT-2 model
image_encoder_model = "google/vit-base-patch16-224-in21k"
text_decode_model = "gpt2"
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(image_encoder_model, text_decode_model)
model.to(device)

# Load feature extractor and tokenizer for the respective models
feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)
tokenizer = AutoTokenizer.from_pretrained(text_decode_model)
tokenizer.pad_token = tokenizer.eos_token

# Set model configurations
model.config.eos_token_id = tokenizer.eos_token_id
model.config.decoder_start_token_id = tokenizer.bos_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.max_length = 20
model.config.min_length = 6
model.config.length_penalty = 1.3
model.config.num_beams = 3

# # Define paths for the Flickr8k dataset
# CAPTIONS_FILE = '/content/drive/MyDrive/kaggle/input/flickr8k/captions.txt'
# IMAGES_DIR = '/content/drive/MyDrive/kaggle/input/flickr8k/Images'
# Define paths for the Flickr8k dataset
CAPTIONS_FILE = "../input/flickr8k/captions.txt"
IMAGES_DIR = '../input/flickr8k/Images'

# Load the captions file into a pandas DataFrame
df = pd.read_csv(CAPTIONS_FILE)
df['image'] = df['image'].apply(lambda x: f'{IMAGES_DIR}/{x}')

java -version

print(df)

# Split the dataset
total_length = len(df)
train_size = int(total_length * 0.7)
val_size = int(total_length * 0.2)
test_size = total_length - train_size - val_size

train_df = df.iloc[:train_size, :]
val_df = df.iloc[train_size:train_size + val_size, :]
test_df = df.iloc[train_size + val_size:, :]

# Convert to HuggingFace Dataset
processed_train = Dataset.from_pandas(train_df)
processed_val = Dataset.from_pandas(val_df)
processed_test = Dataset.from_pandas(test_df)

# Dataset class definition
class Flickr8kDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, feature_extractor, tokenizer, max_target_length=128):
        self.dataset = dataset
        self.feature_extractor = feature_extractor
        self.tokenizer = tokenizer
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image_path = self.dataset[idx]['image']
        caption = self.dataset[idx]['caption']

        # Load the image and preprocess it using the feature extractor
        image = Image.open(image_path).convert("RGB")
        encoder_input = self.feature_extractor(images=image, return_tensors="pt")

        # Tokenize the caption
        labels = self.tokenizer(caption, max_length=self.max_target_length, padding="max_length", truncation=True, return_tensors="pt")

        # Move tensors to GPU
        pixel_values = encoder_input.pixel_values.squeeze(0)
        labels = labels.input_ids.flatten()

        return {"labels": labels, "pixel_values": pixel_values}

# Create PyTorch Datasets
train_data = Flickr8kDataset(processed_train, feature_extractor, tokenizer)
val_data = Flickr8kDataset(processed_val, feature_extractor, tokenizer)
test_data = Flickr8kDataset(processed_test, feature_extractor, tokenizer)

# DataLoader settings with pin_memory=False
train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4, pin_memory=False)
val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=4, pin_memory=False)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4, pin_memory=False)

print(train_size)

from transformers import TrainerCallback, Trainer
# EarlyStoppingCallback 설정
early_stopping = EarlyStoppingCallback(early_stopping_patience=3)
# Custom Callback 정의
class CustomCallback(TrainerCallback):
    def on_log(self, args, state, control, **kwargs):
        train_loss = next((item.get('loss') for item in state.log_history if 'loss' in item), None)
        eval_loss = next((item.get('eval_loss') for item in state.log_history if 'eval_loss' in item), None)
        print(f"Log Step {state.global_step}: Train Loss = {train_loss}, Eval Loss = {eval_loss}")
# Training arguments
train_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    num_train_epochs=10,
    logging_steps=1000000,  # Reduced logging steps for faster training
    warmup_steps=128,
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    remove_unused_columns=False,
    save_total_limit=2,
    output_dir="/content/drive/MyDrive/results",
    fp16=True
)

# Define the metrics function
def compute_metrics(eval_preds):
    metric = evaluate.load("rouge")
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {k: round(v * 100, 4) for k, v in result.items()}
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    return result

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]
    return preds, labels

# Create the trainer
trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=tokenizer,
    args=train_args,
    compute_metrics=compute_metrics,
    train_dataset=train_data,
    eval_dataset=val_data,
    data_collator=default_data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)



!pip install rouge_score

# Train the model
trainer.train()

# Save the trained model
trainer.save_model("/content/drive/MyDrive/vitgpt-2")

# Evaluate the model on the test set
test_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy="no",
    per_device_eval_batch_size=32,
    remove_unused_columns=False,
    output_dir="/content/drive/MyDrive/test_results"
)

# Create the trainer for testing
test_trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=tokenizer,
    args=test_args,
    eval_dataset=test_data,
    data_collator=default_data_collator
)

# Predict on the test dataset
predictions = test_trainer.predict(test_data)

"""[링크 텍스트](https://)"""



import os
import cv2
import matplotlib.pyplot as plt
from transformers import pipeline
import evaluate
from datasets import Dataset
from nltk.translate.bleu_score import corpus_bleu

# 이미지 캡셔닝 파이프라인 생성
image_captioner = pipeline("image-to-text", model=model, tokenizer=tokenizer, feature_extractor=feature_extractor, device=0)

# 이미지와 캡션을 표시하는 함수
def image_display(filepath, correct_caps, gen_caption, bleu_scores=None, rouge_1=None, rouge_2=None, rouge_l=None, font_size=7, dpi=300, save_img=False):
    img_color = cv2.imread(filepath, 1)
    plt.axis("off")
    plt.imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))

    correct_caps_string = f"> Correct Captions:\n{correct_caps}"
    generated_caption_string = f"> Generated Caption:\n{gen_caption[0]['generated_text']}"

    scores_string = "> Scores:\n"
    if bleu_scores is not None:
        scores_string += f"BLEU-1: {bleu_scores['bleu_1']:.4f} | "
        scores_string += f"BLEU-2: {bleu_scores['bleu_2']:.4f} | "
    if rouge_1 is not None and rouge_2 is not None and rouge_l is not None:
        scores_string += f"ROUGE-1: {rouge_1:.4f} | ROUGE-2: {rouge_2:.4f} | ROUGE-L: {rouge_l:.4f}"

    title_string = f"{correct_caps_string}\n\n{generated_caption_string}\n\n{scores_string.strip('| ')}"
    plt.title(title_string, fontsize=font_size, wrap=True, loc='left')

    if save_img:
        plt.savefig(f"{os.path.splitext(os.path.basename(filepath))[0]}_generation", bbox_inches='tight', dpi=dpi)
    plt.show()

# BLEU 점수 계산 함수
def compute_bleu(predictions, references):
    # 다양한 가중치를 사용한 BLEU 점수 계산
    bleu_score_1 = corpus_bleu(references, predictions, weights=(0.3, 0.3, 0.3, 0))
    bleu_score_2 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))

    # 점수들을 딕셔너리로 반환
    return {
        'bleu_1': bleu_score_1,
        'bleu_2': bleu_score_2
    }

# ROUGE 점수 계산 함수
def compute_rouge(predictions, references):
    rouge_metric = evaluate.load("rouge")
    results = rouge_metric.compute(predictions=predictions, references=references)
    rouge_1 = results['rouge1']
    rouge_2 = results['rouge2']
    rouge_l = results['rougeL']
    return rouge_1, rouge_2, rouge_l

# BLEU 및 ROUGE 점수를 계산하는 함수
def get_scores(correct_caps, gen_caption):
    correct_caps_list = [correct_caps.split("\n")]
    predictions = [gen_caption]

    # BLEU 점수 계산
    bleu_scores = compute_bleu(predictions, correct_caps_list)

    # ROUGE 점수 계산
    rouge_1, rouge_2, rouge_l = compute_rouge(predictions, correct_caps_list)

    return bleu_scores, rouge_1, rouge_2, rouge_l

# 이미지에 대한 캡션을 생성하고 BLEU 및 ROUGE 점수를 계산하여 출력하는 함수
def get_caption_for_image(filepath, correct_caps, font_size=7, dpi=300, save_img=False):
    gen_caption = image_captioner(filepath)
    bleu_scores, rouge_1, rouge_2, rouge_l = get_scores(correct_caps, gen_caption[0]['generated_text'])
    image_display(filepath, correct_caps, gen_caption, bleu_scores, rouge_1, rouge_2, rouge_l, font_size, dpi, save_img)

# 테스트 데이터 그룹화
grouped_test_df = test_df.groupby('image')['caption'].agg(lambda x: '\n'.join(x)).reset_index()
grouped_test_ds = Dataset.from_pandas(grouped_test_df)

# 예제 이미지를 생성하고 결과를 표시
examples = [10, 11, 12, 13, 14, 15, 16,17, 18, 19]
for i in examples:
    instance = grouped_test_ds.__getitem__(i)
    image_path = instance['image']
    caption = instance['caption']
    print(f"\n\nImage {i} --> {os.path.splitext(os.path.basename(image_path))[0]}")
    get_caption_for_image(image_path, caption, font_size=7, dpi=500, save_img=True)

# 모든 테스트 데이터에 대해 BLEU 및 ROUGE 점수의 평균 계산
bleu_scores_list = []
rouge_1_list = []
rouge_2_list = []
rouge_l_list = []

for i in range(len(grouped_test_ds)):
    instance = grouped_test_ds.__getitem__(i)
    image_path = instance['image']
    caption = instance['caption']
    gen_caption = image_captioner(image_path)

    bleu_scores, rouge_1, rouge_2, rouge_l = get_scores(caption, gen_caption[0]['generated_text'])

    # 점수를 리스트에 저장
    bleu_scores_list.append(bleu_scores)
    rouge_1_list.append(rouge_1)
    rouge_2_list.append(rouge_2)
    rouge_l_list.append(rouge_l)

# 각 점수의 평균값 계산
avg_bleu_1 = np.mean([score['bleu_1'] for score in bleu_scores_list])
avg_bleu_2 = np.mean([score['bleu_2'] for score in bleu_scores_list])
avg_rouge_1 = np.mean(rouge_1_list)
avg_rouge_2 = np.mean(rouge_2_list)
avg_rouge_l = np.mean(rouge_l_list)

print(f"\nAverage BLEU-1: {avg_bleu_1:.4f}")
print(f"Average BLEU-2: {avg_bleu_2:.4f}")
print(f"Average ROUGE-1: {avg_rouge_1:.4f}")
print(f"Average ROUGE-2: {avg_rouge_2:.4f}")
print(f"Average ROUGE-L: {avg_rouge_l:.4f}")